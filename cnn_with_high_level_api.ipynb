{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_with_high_level_api",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJWepkowclcSdYj/mZHV3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanocha/try-tensorflow/blob/master/cnn_with_high_level_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QbaqVsOXw1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class ConvNN(object):\n",
        "  def __init__(\n",
        "    self,\n",
        "    batchsize=64,\n",
        "    epochs=20,\n",
        "    learning_rate=1e-4,\n",
        "    dropout_rate=0.5,\n",
        "    shuffle=True,\n",
        "    random_seed=None,  \n",
        "  ):\n",
        "    np.random.seed = batchsize\n",
        "    self.batch_size = batchsize\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "    g = tf.Graph()\n",
        "    with g.as_default():\n",
        "      tf.set_random_seed(random_seed)\n",
        "      self.build()\n",
        "      self.init_op = tf.global_variables_initializer()\n",
        "      self.saver = tf.train.Saver()\n",
        "\n",
        "    self.sess = tf.Session(graph=g)\n",
        "\n",
        "  def build(self):\n",
        "    tf_x = tf.placeholder(tf.float32, shape=[None, 784], name='tf_x')\n",
        "    tf_y = tf.placeholder(tf.int32, shape=[None], name='tf_y')\n",
        "    is_train = tf.placeholder(tf.bool, shape=(), name='is_train')\n",
        "\n",
        "    tf_x_image = tf.reshape(\n",
        "        tf_x,\n",
        "        shape=[-1, 28, 28, 1],\n",
        "        name='input_x_2dimages',\n",
        "    )\n",
        "\n",
        "    tf_y_onehot = tf.one_hot(\n",
        "        indices=tf_y,\n",
        "        depth=10,\n",
        "        dtype=tf.float32,\n",
        "        name='input_y_onehot',\n",
        "    )\n",
        "\n",
        "    h1 = tf.layers.conv2d(\n",
        "        tf_x_image,\n",
        "        kernel_size=(5, 5),\n",
        "        filters=32,\n",
        "        activation=tf.nn.relu,\n",
        "    )\n",
        "\n",
        "    h1_pool = tf.layers.max_pooling2d(\n",
        "        h1,\n",
        "        pool_size=(2, 2),\n",
        "        strides=(2, 2),\n",
        "    )\n",
        "\n",
        "    h2 = tf.layers.conv2d(\n",
        "        h1_pool,\n",
        "        kernel_size=(5, 5),\n",
        "        filters=64,\n",
        "        activation=tf.nn.relu,\n",
        "    )\n",
        "\n",
        "    h2_pool = tf.layers.max_pooling2d(\n",
        "        h2,\n",
        "        pool_size=(2, 2),\n",
        "        strides=(2, 2),\n",
        "    )\n",
        "\n",
        "    input_shape = h2_pool.get_shape().as_list()\n",
        "    n_input_units = np.prod(input_shape[1:])\n",
        "    h2_pool_flat = tf.reshape(h2_pool, shape=[-1, n_input_units])\n",
        "    h3 = tf.layers.dense(h2_pool_flat, 1024, activation=tf.nn.relu)\n",
        "\n",
        "    h3_drop = tf.layers.dropout(\n",
        "        h3,\n",
        "        rate=self.dropout_rate,\n",
        "        training=is_train,\n",
        "    )\n",
        "\n",
        "    h4 = tf.layers.dense(\n",
        "        h3_drop,\n",
        "        10,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    predictions = {\n",
        "        'probabilities': tf.nn.softmax(h4, name='probabilities'),\n",
        "        'labels': tf.cast(tf.argmax(h4, axis=1), tf.int32, name='labels'),\n",
        "    }\n",
        "\n",
        "    cross_entropy_loss = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=h4,\n",
        "            labels=tf_y_onehot,\n",
        "        ),\n",
        "        name='cross_entropy_loss',\n",
        "    )\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "    optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
        "\n",
        "    correct_predictions = tf.equal(predictions['labels'], tf_y, name='correct_preds')\n",
        "    accuracy = tf.reduce_mean(\n",
        "        tf.cast(correct_predictions, tf.float32),\n",
        "        name='accuracy',\n",
        "    )\n",
        "\n",
        "  def save(self, epoch, path='./tflayers_model/'):\n",
        "    if not os.path.isdir(path):\n",
        "      os.makedirs(path)\n",
        "\n",
        "    self.saver.save(self.sess, os.path.join(path, 'model.ckpt'), global_step=epoch)\n",
        "\n",
        "  def load(self, path, epoch):\n",
        "    self.saver.restore(self.sess, os.path.join(path, 'model.ckpt-%d' % epoch))\n",
        "\n",
        "\n",
        "  def train(self, training_set, validation_set=None, initialize=True):\n",
        "    if initialize:\n",
        "      self.sess.run(self.init_op)\n",
        "\n",
        "    self.train_cost_ = []\n",
        "    X_data = np.array(training_set[0])\n",
        "    y_data = np.array(training_set[1])\n",
        "\n",
        "    for epoch in range(1, self.epochs + 1):\n",
        "      batch_gen = batch_generator(X_data, y_data, shuffle=self.shuffle)\n",
        "      avg_loss = 0.0\n",
        "\n",
        "      for i, (batch_X, batch_y) in enumerate(batch_gen):\n",
        "        feed = {\n",
        "            'tf_x:0': batch_X,\n",
        "            'tf_y:0': batch_y,\n",
        "            'is_train:0': True,\n",
        "        }\n",
        "\n",
        "        loss, _ = self.sess.run(\n",
        "            ['cross_entropy_loss:0', 'train_op'],\n",
        "            feed_dict=feed,\n",
        "        )\n",
        "\n",
        "        avg_loss += loss\n",
        "\n",
        "      print('epoch %02d training avg loss: %7.3f' % (epoch, avg_loss), end=' ')\n",
        "\n",
        "      if validation_set is not None:\n",
        "        feed = {\n",
        "            'tf_x:0': validation_set[0],\n",
        "            'tf_y:0': validation_set[1],\n",
        "            'is_train:0': False,\n",
        "        }\n",
        "\n",
        "        valid_acc = self.sess.run('accuracy:0', feed_dict=feed)\n",
        "        print('validation acc: %7.3f' % valid_acc)\n",
        "      else:\n",
        "        print()\n",
        "\n",
        "\n",
        "  def predict(self, X_test, return_proba=False):\n",
        "    feed = {\n",
        "        'tf_x:0': X_test,\n",
        "        'is_train:0': False,\n",
        "    }\n",
        "\n",
        "    if return_proba:\n",
        "      return self.sess.run('probabilities:0', feed_dict=feed)\n",
        "    else:\n",
        "      return self.sess.run('labels:0', feed_dict=feed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE3IzL4hdtar",
        "colab_type": "code",
        "outputId": "e091f687-5a27-447e-9895-47002978d232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(X_data, y_data), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train, y_train = X_data[:50000, :], y_data[:50000]\n",
        "X_valid, y_valid = X_data[50000:, :], y_data[50000:]\n",
        "\n",
        "X_train = X_train.reshape([50000, -1])\n",
        "X_valid = X_valid.reshape([10000, -1])\n",
        "X_test = X_test.reshape([10000, -1])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "mean_vals = np.mean(X_train, axis=0)\n",
        "std_val = np.std(X_train)\n",
        "\n",
        "X_train_centered = (X_train - mean_vals) / std_val\n",
        "X_valid_centered = X_valid - mean_vals\n",
        "X_test_centered = (X_test - mean_vals) / std_val\n",
        "\n",
        "print(X_train_centered.shape, y_train.shape)\n",
        "print(X_valid_centered.shape, y_valid.shape)\n",
        "print(X_test_centered.shape, y_test.shape)\n",
        "\n",
        "print(y_train)\n",
        "print(y_train.shape)\n",
        "\n",
        "def batch_generator(X, y, batch_size=64, shuffle=False, random_seed=None):\n",
        "  idx = np.arange(y.shape[0])\n",
        "\n",
        "  if shuffle:\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    rng.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "\n",
        "  for i in range(0, X.shape[0], batch_size):\n",
        "    yield(\n",
        "        X[i:i+batch_size, :],\n",
        "        y[i:i+batch_size],\n",
        "    )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(50000, 784)\n",
            "(10000, 784)\n",
            "(10000, 784)\n",
            "(50000, 784) (50000,)\n",
            "(10000, 784) (10000,)\n",
            "(10000, 784) (10000,)\n",
            "[5 0 4 ... 8 4 8]\n",
            "(50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZp2ZBbOd2HD",
        "colab_type": "code",
        "outputId": "98154e94-aad4-49b9-ae74-a344521cbb38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "cnn = ConvNN(random_seed=123)\n",
        "\n",
        "cnn.train(training_set=(X_train_centered, y_train), validation_set=(X_valid_centered, y_valid))\n",
        "cnn.save"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 01 training avg loss: 269.957 validation acc:   0.973\n",
            "epoch 02 training avg loss:  74.570 validation acc:   0.983\n",
            "epoch 03 training avg loss:  50.755 validation acc:   0.985\n",
            "epoch 04 training avg loss:  38.474 validation acc:   0.988\n",
            "epoch 05 training avg loss:  31.440 validation acc:   0.989\n",
            "epoch 06 training avg loss:  27.901 validation acc:   0.989\n",
            "epoch 07 training avg loss:  21.703 validation acc:   0.989\n",
            "epoch 08 training avg loss:  20.310 validation acc:   0.991\n",
            "epoch 09 training avg loss:  16.473 validation acc:   0.991\n",
            "epoch 10 training avg loss:  14.836 validation acc:   0.991\n",
            "epoch 11 training avg loss:  13.473 validation acc:   0.991\n",
            "epoch 12 training avg loss:  11.598 validation acc:   0.991\n",
            "epoch 13 training avg loss:  10.203 validation acc:   0.989\n",
            "epoch 14 training avg loss:   9.682 validation acc:   0.992\n",
            "epoch 15 training avg loss:   8.071 validation acc:   0.991\n",
            "epoch 16 training avg loss:   6.575 validation acc:   0.992\n",
            "epoch 17 training avg loss:   7.055 validation acc:   0.992\n",
            "epoch 18 training avg loss:   5.493 validation acc:   0.992\n",
            "epoch 19 training avg loss:   4.830 validation acc:   0.991\n",
            "epoch 20 training avg loss:   4.503 validation acc:   0.992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method ConvNN.save of <__main__.ConvNN object at 0x7f639af73da0>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}